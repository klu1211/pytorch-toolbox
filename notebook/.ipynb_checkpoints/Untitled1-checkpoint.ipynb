{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_string = {\n",
    "    0:  'Nucleoplasm',\n",
    "    1:  'Nuclear membrane',\n",
    "    2:  'Nucleoli',   \n",
    "    3:  'Nucleoli fibrillar center',\n",
    "    4:  'Nuclear speckles',\n",
    "    5:  'Nuclear bodies',\n",
    "    6:  'Endoplasmic reticulum',   \n",
    "    7:  'Golgi apparatus',\n",
    "    8:  'Peroxisomes',\n",
    "    9:  'Endosomes',\n",
    "    10:  'Lysosomes',\n",
    "    11:  'Intermediate filaments',\n",
    "    12:  'Actin filaments',\n",
    "    13:  'Focal adhesion sites',   \n",
    "    14:  'Microtubules',\n",
    "    15:  'Microtubule ends',  \n",
    "    16:  'Cytokinetic bridge',   \n",
    "    17:  'Mitotic spindle',\n",
    "    18:  'Microtubule organizing center',  \n",
    "    19:  'Centrosome',\n",
    "    20:  'Lipid droplets',\n",
    "    21:  'Plasma membrane',   \n",
    "    22:  'Cell junctions', \n",
    "    23:  'Mitochondria',\n",
    "    24:  'Aggresome',\n",
    "    25:  'Cytosol',\n",
    "    26:  'Cytoplasmic bodies',   \n",
    "    27:  'Rods & rings'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "from src.data import DataPaths, Image, open_rgby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_toolbox.fastai.fastai import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = [Path(DataPaths.TRAIN_IMAGES, img_id) for img_id in np.unique([p.name[:36] for p in DataPaths.TRAIN_IMAGES.glob(\"*\")])]\n",
    "test_paths = [Path(DataPaths.TEST_IMAGES, img_id) for img_id in np.unique([p.name[:36] for p in DataPaths.TEST_IMAGES.glob(\"*\")])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = random.sample(train_paths, 1)[0]\n",
    "sample_img = open_rgby(sample_path)\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, Flip, OneOf, Compose, ElasticTransform,\n",
    "    Resize\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "def strong_aug(p=0.5):\n",
    "    return Compose([\n",
    "        RandomRotate90(),\n",
    "        Flip(),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, always_apply=True),\n",
    "        Resize(height=256, width=256, always_apply=True)\n",
    "#         ElasticTransform(p=0.5),\n",
    "    ], p=p)\n",
    "\n",
    "def albumentations_transform_wrapper(image, augment_fn):\n",
    "    augmentation = augment_fn(image=image)\n",
    "    return augmentation['image']\n",
    "\n",
    "augment_fn = partial(albumentations_transform_wrapper, augment_fn=strong_aug(p=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(augment_fn(sample_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(2, 4, figsize=(12, 12))\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.imshow(augment_fn(sample_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(DataPaths.TRAIN_LABELS)\n",
    "labels_df['Target'] = [[int(i) for i in s.split()] for s in labels_df['Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = labels_df.loc[labels_df[\"Id\"] == sample_path.name]['Target'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate one hot labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_one_hot = []\n",
    "train_labels = labels_df['Target']\n",
    "for labels in tqdm_notebook(train_labels):\n",
    "    one_hot = np.zeros((28))\n",
    "    for label in labels:\n",
    "        one_hot[label] = 1\n",
    "    train_labels_one_hot.append(one_hot.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels[0]\n",
    "train_labels_one_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now generate training input/label pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = sorted([Path(DataPaths.TRAIN_IMAGES, img_id) for img_id in np.unique([p.name[:36] for p in DataPaths.TRAIN_IMAGES.glob(\"*\")])], key=lambda p: p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_df.sort_values([\"Id\"], ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.array([p.name for p in train_paths]) == labels_df[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = labels_df[\"Target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_paths) == len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from src.data import ProteinClassificationDataset\n",
    "from pytorch_toolbox.utils.image import normalize, denormalize, tensor2img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_channel_image_net_stats = {\n",
    "    'mean': [0.485, 0.456, 0.406, 0.485],\n",
    "    'sd': [0.229, 0.224, 0.224, 0.229]\n",
    "}\n",
    "four_channel_image_net_normalize = partial(normalize, **four_channel_image_net_stats)\n",
    "four_channel_image_net_denormalize = partial(denormalize, **four_channel_image_net_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ProteinClassificationDataset(train_paths, labels=train_labels_one_hot, augment_fn=augment_fn, normalization_fn=four_channel_image_net_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(ds))\n",
    "inp, name, label = batch['input'], batch['name'], batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor2img(inp, denorm_fn=four_channel_image_net_denormalize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from pytorch_toolbox.fastai_extensions.loss import LossWrapper, FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from pytorch_toolbox.fastai_extensions.basic_data import DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_split_method = partial(ShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split, X=train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(split_method):\n",
    "    return next(iter(split_method()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_bunch(split_method, augment_fn, normalization_fn, num_workers=fastai.defaults.cpus, train_bs=64, val_bs=None, test_bs=None):\n",
    "    train_idx, val_idx = get_splits(split_method)\n",
    "    if val_bs is None: val_bs = train_bs * 2\n",
    "    if test_bs is None: test_bs = train_bs * 2\n",
    "    \n",
    "    train_ds = ProteinClassificationDataset(inputs=np.array(train_paths)[train_idx], \n",
    "                                            labels=np.array(train_labels_one_hot)[train_idx],\n",
    "                                            augment_fn=augment_fn,\n",
    "                                            normalization_fn=normalization_fn)\n",
    "    val_ds = ProteinClassificationDataset(inputs=np.array(train_paths)[val_idx],\n",
    "                                          labels=np.array(train_labels_one_hot)[val_idx],\n",
    "                                          normalization_fn=normalization_fn)\n",
    "    test_ds = ProteinClassificationDataset(inputs=np.array(test_paths),\n",
    "                                           normalization_fn=normalization_fn)\n",
    "    \n",
    "    data = DataBunch.create(train_ds, val_ds, test_ds,\n",
    "                            num_workers=num_workers,\n",
    "                            collate_fn=ProteinClassificationDataset.collate_fn,\n",
    "                            train_bs=train_bs,\n",
    "                            val_bs=val_bs,\n",
    "                            test_bs=test_bs)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_data_bunch(split_method=shuffle_split_method,\n",
    "                         augment_fn=augment_fn,\n",
    "                         normalization_fn=four_channel_image_net_normalize,\n",
    "                         num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, name, label = batch[0], batch[1]['name'], batch[1]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor2img(inp[4], denorm_fn=four_channel_image_net_denormalize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_toolbox.fastai.fastai as fastai\n",
    "from pytorch_toolbox.fastai_extensions.basic_train import Learner\n",
    "from src.models import cbam_resnet50_four_channel_input, resnet50_four_channel_input, resnet34_four_channel_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize model pretrained resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34_four_channel_input(pretrained=True)\n",
    "n_starting_layers = len(fastai.flatten_model(model[:6]))\n",
    "n_middle_layers = len(fastai.flatten_model(model[6:9]))\n",
    "n_head = len(fastai.flatten_model(model[9:]))\n",
    "layer_groups = fastai.split_model_idx(model, [n_starting_layers, n_starting_layers + n_middle_layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize model pretrained resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resnet50_four_channel_input(pretrained=True)\n",
    "# n_starting_layers = len(fastai.flatten_model(model[:6]))\n",
    "# n_middle_layers = len(fastai.flatten_model(model[6:9]))\n",
    "# n_head = len(fastai.flatten_model(model[9:]))\n",
    "# layer_groups = fastai.split_model_idx(model, [n_starting_layers, n_starting_layers + n_middle_layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize model pretrained resnet CBAM50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = cbam_resnet50_four_channel_input(pretrained=False)\n",
    "# n_starting_layers = len(fastai.flatten_model(model[:6]))\n",
    "# n_middle_layers = len(fastai.flatten_model(model[6:9]))\n",
    "# n_head = len(fastai.flatten_model(model[9:]))\n",
    "# layer_groups = fastai.split_model_idx(model, [n_starting_layers, n_starting_layers + n_middle_layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For extracting relevant information from output of our data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_toolbox.fastai_extensions.callbacks import NameExtractionTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For saving the results of our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputRecorder(fastai.LearnerCallback):\n",
    "    _order = -10\n",
    "    def __init__(self, learn):\n",
    "        super().__init__(learn)\n",
    "        self.history = defaultdict(list)\n",
    "        self.phase = None\n",
    "        self.current_batch = dict()\n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, epoch, train, **kwargs):\n",
    "        if train:\n",
    "            self.phase = 'TRAIN'\n",
    "        else:\n",
    "            label = last_target.get('label')\n",
    "            if label is not None:\n",
    "                self.phase = 'VAL'\n",
    "            else:\n",
    "                self.phase = 'TEST'\n",
    "        self.key = (self.phase, epoch)\n",
    "        inputs = tensor2img(last_input, denorm_fn=four_channel_image_net_denormalize)\n",
    "        self.current_batch['input'] = inputs\n",
    "        self.current_batch['name'] = last_target['name']\n",
    "        if self.phase == 'TRAIN' or self.phase == 'VAL':\n",
    "            label = to_numpy(last_target['label'])\n",
    "            self.current_batch['label'] = label\n",
    "\n",
    "    def on_loss_begin(self, last_output, epoch, **kwargs):\n",
    "        model_output = to_numpy(last_output)\n",
    "        self.current_batch['prediction_probs'] = model_output\n",
    "        classes = np.where(model_output > 0.5)[0]\n",
    "        prediction = model_output.copy()\n",
    "        prediction[prediction < 0.5] = 0\n",
    "        prediction[prediction >= 0.5] = 1\n",
    "        self.current_batch['prediction'] = prediction\n",
    "    \n",
    "    def on_batch_end(self, epoch, **kwargs):\n",
    "        self.current_batch['loss'] = to_numpy(self.learn.loss_func.focal_loss.loss)\n",
    "        prediction = self.current_batch['prediction']\n",
    "        label = self.current_batch['label']\n",
    "        n_classes = label.shape[-1]\n",
    "        indices_to_keep = np.where((prediction == label).sum(axis=1) != n_classes)[0]\n",
    "        \n",
    "        if self.phase == \"VAL\":\n",
    "            for idx in indices_to_keep:\n",
    "                sample_to_save = dict()\n",
    "                for k, v in self.current_batch.items():\n",
    "                    sample_to_save[k] = v[idx]\n",
    "                self.history[self.key].append(sample_to_save)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define metric to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds,targs,th=0.0):\n",
    "    preds = (preds > th).int()\n",
    "    targs = targs.int()\n",
    "    return (preds==targs).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(data,\n",
    "                  layer_groups=layer_groups,\n",
    "                  model=model, \n",
    "                  loss_func=LossWrapper([\n",
    "                      FocalLoss()\n",
    "                  ]),\n",
    "                  callbacks=[NameExtractionTrainer()],\n",
    "                  callback_fns=[OutputRecorder],\n",
    "                  metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "learner.lr_find(start_lr=[1e-5] * 3, end_lr=[10] * 3, num_it=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model for real this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(cyc_len=3, max_lr=[2e-2] * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a recorder for the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_toolbox.utils.core import to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRecorder(fastai.Callback):\n",
    "    _order = -10\n",
    "    def __init__(self):\n",
    "        self.names = []\n",
    "        self.prob_preds = []\n",
    "    \n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "#         inputs = tensor2img(last_input, denorm_fn=image_net_denormalize)\n",
    "#         self.inputs.extend(inputs)\n",
    "        self.names.extend(last_target['name'])\n",
    "            \n",
    "    def on_loss_begin(self, last_output, **kwargs):\n",
    "        prob_pred = to_numpy(torch.sigmoid(last_output))\n",
    "        self.prob_preds.extend(prob_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run prediction on learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recorder = TestRecorder()\n",
    "learner.predict_on_dl(dl=learner.data.test_dl, callbacks=[test_recorder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_soft(preds,targs,th=0.5,d=50.0):\n",
    "    preds = sigmoid_np(d*(preds - th))\n",
    "    targs = targs.astype(np.float)\n",
    "    score = 2.0*(preds*targs).sum(axis=0)/((preds+targs).sum(axis=0) + 1e-6)\n",
    "    return score\n",
    "\n",
    "def fit_val(x,y):\n",
    "    params = 0.5*np.ones(len(name_label_dict))\n",
    "    wd = 1e-5\n",
    "    error = lambda p: np.concatenate((F1_soft(x,y,p) - 1.0,\n",
    "                                      wd*(p - 0.5)), axis=None)\n",
    "    p, success = opt.leastsq(error, params)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(test_recorder.prob_preds)\n",
    "predicted = []\n",
    "for i in tqdm_notebook(x):\n",
    "    classes = []\n",
    "    max_class = np.argmax(i)\n",
    "    other_classes = np.where(i > 0.5)[0]\n",
    "    classes.append(max_class)\n",
    "    classes.extend(other_classes)\n",
    "    classes = np.unique(classes)\n",
    "    classes = [str(c) for c in classes]\n",
    "    predicted.append(\" \".join(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": test_recorder.names,\n",
    "    \"Predicted\": predicted\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-protein",
   "language": "python",
   "name": "human-protein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
