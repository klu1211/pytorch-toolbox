{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "import os\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from miniutils.progress_bar import parallel_progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchToolboxLoader(yaml.SafeLoader):\n",
    "    pass\n",
    "\n",
    "def var_constructor(loader, node):\n",
    "    return Variable(name=node.value)\n",
    "\n",
    "@dataclass\n",
    "class Variable:\n",
    "    name: str\n",
    "        \n",
    "def ref_constructor(loader, node):\n",
    "    ref_node_name_and_output_name = node.value.split(\".\")\n",
    "    assert len(ref_node_name_and_output_name) == 2\n",
    "    ref_node_name, output_name = ref_node_name_and_output_name\n",
    "    return Reference(ref_node_name=ref_node_name, output_name=output_name)\n",
    "\n",
    "@dataclass\n",
    "class Reference:\n",
    "    ref_node_name: str\n",
    "    output_name: str\n",
    "        \n",
    "def replace_config_variables(config, resource_key=\"Resources\", variable_key=\"Variables\"):\n",
    "    config = deepcopy(config)\n",
    "    try:\n",
    "        replaced_resources = replace_variables(config[resource_key], config[variable_key])\n",
    "        config[resource_key] = replaced_resources\n",
    "    except KeyError:\n",
    "        return config\n",
    "    return config\n",
    "\n",
    "def replace_variables(resources, variables):\n",
    "    if isinstance(resources, dict):\n",
    "        for name, resource in resources.items():\n",
    "            resources[name] = replace_variables(resource, variables)\n",
    "    elif isinstance(resources, list):\n",
    "        for i, resource in enumerate(resources):\n",
    "            resources[i] = replace_variables(resource, variables)\n",
    "    elif isinstance(resources, Variable):\n",
    "        resources = variables[resources.name]\n",
    "    else:\n",
    "        return resources\n",
    "    return resources\n",
    "\n",
    "\n",
    "# This tells the loader that when it sees \"!path\" it will pass the value proceeding the !path value into the path constructor\n",
    "PyTorchToolboxLoader.add_constructor('!Var', var_constructor)\n",
    "PyTorchToolboxLoader.add_constructor('!Ref', ref_constructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(Path(\"yaml_loader_test_resource.yml\").open(\"r\"), Loader=PyTorchToolboxLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_config = replace_config_variables(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Variables': {'single_variable': 'hello', 'list_variable': ['foo', 'bar']}, 'Resources': {'TestSingleVariableReplacement': {'single_variable': 'hello'}, 'TestListVariableReplacement': {'list_variable': ['foo', 'bar']}, 'TestVariableInListReplacement': ['hello', ['foo', 'bar']], 'TestVariableInListOfDictionaryReplacement': [{'dict_1': 'hello'}, {'dict_2': ['foo', 'bar']}], 'MockReferenceForTest': {'output': ['some_reference']}, 'TestFindReference': {'ref_var': Reference(ref_node_name='MockReferenceForTest', output_name='some_reference')}, 'TestFindReferenceInList': [Reference(ref_node_name='MockReferenceForTest', output_name='some_reference')], 'TestFindReferenceInListOfDictionary': {'ref_var_in_list_of_dictionary': [{'key_1': Reference(ref_node_name='MockReferenceForTest', output_name='some_reference')}]}}}"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSingleVariableReplacement\n",
      "[]\n",
      "TestListVariableReplacement\n",
      "[]\n",
      "TestVariableInListReplacement\n",
      "[]\n",
      "TestVariableInListOfDictionaryReplacement\n",
      "[]\n",
      "MockReferenceForTest\n",
      "[]\n",
      "TestFindReference\n",
      "[Reference(ref_node_name='MockReferenceForTest', output_name='some_reference')]\n",
      "TestFindReferenceInList\n",
      "[Reference(ref_node_name='MockReferenceForTest', output_name='some_reference')]\n",
      "TestFindReferenceInListOfDictionary\n",
      "[Reference(ref_node_name='MockReferenceForTest', output_name='some_reference')]\n"
     ]
    }
   ],
   "source": [
    "for name, resource in config['Resources'].items():\n",
    "    print(name)\n",
    "    print(find_references(resource))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try on original configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Optional, Dict, Any, Iterator, Iterable, Sequence, Union, Callable, Tuple, List, Any, Collection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def load_training_data(root_image_paths, root_label_paths, use_n_samples):\n",
    "    train_df = load_training_data_df(root_image_paths, root_label_paths, use_n_samples)\n",
    "    labels = train_df['Target'].values\n",
    "    labels_one_hot = make_one_hot(labels, n_classes=28)\n",
    "    return np.array(train_df['ImagePath'].values), np.array(labels), np.array(labels_one_hot)\n",
    "\n",
    "def load_training_data_df(root_image_paths, root_label_paths, use_n_samples):\n",
    "    labels_df = load_training_labels(root_label_paths)\n",
    "    labels_df.sort_values([\"Id\"], ascending=[True], inplace=True)\n",
    "    labels_df_sorted_by_id = labels_df\n",
    "\n",
    "    # As some duplicate images were removed, we only use the images that have labels\n",
    "    image_paths = load_training_images(root_image_paths)\n",
    "    image_paths_with_labels = filter_image_paths_with_labels(image_paths, labels_df)\n",
    "\n",
    "    # Sort by ID so that the labels and the image matches up\n",
    "    train_df = labels_df_sorted_by_id\n",
    "    image_paths_sorted_by_id = sorted(image_paths_with_labels, key=lambda path: path.stem)\n",
    "    train_df[\"ImagePath\"] = image_paths_sorted_by_id\n",
    "\n",
    "    if use_n_samples:\n",
    "        train_df = train_df.sample(use_n_samples)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def load_training_labels(training_labels_path):\n",
    "    labels_df = pd.read_csv(training_labels_path)\n",
    "    labels_df['Target'] = [[int(i) for i in s.split()] for s in labels_df['Target']]\n",
    "    labels_df['TargetTuple'] = [tuple(t) for t in labels_df['Target']]\n",
    "    return labels_df\n",
    "\n",
    "def load_training_images(training_images_path):\n",
    "    image_paths = []\n",
    "    for p in listify(training_images_path):\n",
    "        image_paths.extend(Path(p).glob(\"*\"))\n",
    "    return image_paths\n",
    "\n",
    "def filter_image_paths_with_labels(image_paths, labels_df):\n",
    "    # We use a Counter to filter in O(n) instead of O(n^2) time\n",
    "    image_id_with_labels_lookup = Counter(labels_df['Id'])\n",
    "    image_paths_used_for_training = [Path(p) for p in image_paths if\n",
    "                                     image_id_with_labels_lookup.get(Path(p).stem) is not None]\n",
    "    return np.array(image_paths_used_for_training)\n",
    "\n",
    "def add_number_of_labels_column(train_df):\n",
    "    train_df.sort_values([\"TargetTuple\"], ascending=[True])\n",
    "    label_counts = Counter([tuple(l) for l in train_df['Target'].values])\n",
    "    train_df['Count'] = [label_counts[tuple(l)] for l in train_df['Target']]\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def add_one_hot_labels_index_column(train_df):\n",
    "    target_tuple_to_label = {v: k for k, v in enumerate(train_df['TargetTuple'].unique())}\n",
    "    train_df['OneHotLabelIndex'] = train_df['TargetTuple'].map(lambda x: target_tuple_to_label[x])\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def load_testing_data(root_image_paths, use_n_samples=None):\n",
    "    X = sorted(list(Path(root_image_paths).glob(\"*\")), key=lambda p: p.stem)\n",
    "    if use_n_samples is not None:\n",
    "        X = X[:use_n_samples]\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "def make_one_hot(labels, n_classes=28):\n",
    "    one_hots = []\n",
    "    for label in labels:\n",
    "        one_hot = np.zeros(n_classes)\n",
    "        for label_idx in label:\n",
    "            one_hot[label_idx] = 1\n",
    "        one_hots.append(one_hot.astype(np.float32))\n",
    "    return one_hots\n",
    "\n",
    "def calculate_mean_and_std_for_dataset(some_var, data_paths):\n",
    "    flattened_data_paths = list(chain(*data_paths))\n",
    "    means, stds = list(zip(*parallel_progbar(calculate_mean_and_std, flattened_data_paths)))\n",
    "    mean = np.stack(means).mean(axis=0)\n",
    "    std = np.stack(stds).mean(axis=0)\n",
    "    logging.info(f\"Mean of dataset is: {mean}\")\n",
    "    logging.info(f\"Standard deviation of dataset is: {std}\")\n",
    "    return mean, std\n",
    "\n",
    "def calculate_mean_and_std(img_path):\n",
    "    img = open_numpy(img_path, with_image_wrapper=True).tensor\n",
    "    mean = np.mean(img.numpy(), axis=(1, 2))\n",
    "    std = np.std(img.numpy(), axis=(1, 2))\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "lookup = {\n",
    "    \"load_testing_data\": load_testing_data,\n",
    "    \"load_training_data\": load_training_data,\n",
    "    \"calculate_mean_and_std_for_dataset\": calculate_mean_and_std_for_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import networkx as nx\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, graph, config):\n",
    "        self.graph = graph\n",
    "        self.config = config\n",
    "        self.shared_state = {\n",
    "            \"config\": deepcopy(config)\n",
    "        }\n",
    "        \n",
    "    @classmethod\n",
    "    def create_graph_from_config(cls, config):\n",
    "        assert config.get(\"Resources\") is not None, \"There is no Resources key in the configuration file\"\n",
    "        graph = nx.DiGraph()\n",
    "        flattened_resources = flatten_dict(config[\"Resources\"])\n",
    "        graph = cls._add_nodes_to_graph(graph, flattened_resources)\n",
    "        graph = cls._add_edges_to_graph(graph)\n",
    "        return cls(graph, config)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _add_nodes_to_graph(graph, resources):\n",
    "        for name, resource in resources.items():\n",
    "            references = find_references(resource)\n",
    "            properties = load_properties_with_default_values(resource[\"properties\"])\n",
    "            node = Node(name=name, references=references, **properties)\n",
    "            graph.add_node(name, node=node)\n",
    "        return graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_edges_to_graph(graph):\n",
    "        for name, node_wrapper in graph.nodes(data=True):\n",
    "            node = node_wrapper[\"node\"]\n",
    "            for reference in node.references:\n",
    "                referenced_node_name = reference.ref_node_name\n",
    "                assert referenced_node_name in pipeline_graph.nodes, f\"The reference: {referenced_node_name} in node: {node.name} does not exist\"\n",
    "                graph.add_edge(referenced_node_name, name)\n",
    "        return graph\n",
    "    \n",
    "    def run(self, to_node=None):\n",
    "        for node_name in nx.algorithms.dag.topological_sort(self.graph):\n",
    "            if to_node == node_name:\n",
    "                break\n",
    "            node = self.graph.nodes(data=True)[node_name][\"node\"]\n",
    "            self.run_node(node)\n",
    "            \n",
    "    def run_node(self, node):\n",
    "        replace_arguments(self.graph, node)\n",
    "        node.create_output()\n",
    "\n",
    "def flatten_dict(d):\n",
    "    flattened = dict()\n",
    "    for resources in d.values():\n",
    "        flattened = {**flattened_resources, **resources}\n",
    "    return flattened\n",
    "\n",
    "def find_references(resource):\n",
    "    references = []\n",
    "    if isinstance(resource, dict):\n",
    "        for _, value in resource.items():\n",
    "            references.extend(find_references(value))\n",
    "    elif isinstance(resource, list):\n",
    "        for value in resource:\n",
    "            references.extend(find_references(value))\n",
    "    elif isinstance(resource, Reference):\n",
    "        references.append(resource)\n",
    "    else:\n",
    "        pass\n",
    "    return references\n",
    "\n",
    "def replace_arguments(graph, node):\n",
    "    try:\n",
    "        arguments = node.arguments\n",
    "        if arguments is not None:\n",
    "            reference_replaced_arguments = replace_references(graph, deepcopy(arguments))\n",
    "            node.reference_replaced_arguments = reference_replaced_arguments\n",
    "    except AttributeError:\n",
    "        pass   \n",
    "\n",
    "\n",
    "def replace_references(graph, arguments):\n",
    "    if isinstance(arguments, dict):\n",
    "        for name, argument in arguments.items():\n",
    "            arguments[name] = replace_references(graph, argument)\n",
    "    elif isinstance(arguments, list):\n",
    "        for i, argument in enumerate(arguments):\n",
    "            arguments[i] = replace_references(graph, argument)\n",
    "    elif isinstance(arguments, Reference):\n",
    "        # reassign name it make the intent clearer\n",
    "        reference = arguments\n",
    "        ref_node = graph.nodes(data=True)[reference.ref_node_name][\"node\"]\n",
    "        ref_node_outputs = ref_node.output\n",
    "        assert ref_node_outputs is not None, f\"Node: {reference.ref_node_name} has no output\"\n",
    "        assert reference.output_name in ref_node_outputs, f\"Node: {reference_node_name} has no output named {reference.output_name}\"\n",
    "        arguments = ref_node_outputs[reference.output_name]\n",
    "    else:\n",
    "        return arguments\n",
    "    return arguments\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name, references, pointer, partial, arguments, output_names):\n",
    "        self.name = name\n",
    "        self.references = references\n",
    "        self.pointer = pointer\n",
    "        self.arguments = arguments\n",
    "        self.output_names = output_names\n",
    "        self.partial = partial\n",
    "        self.reference_replaced_arguments = None\n",
    "        self.output = None\n",
    "        \n",
    "    def create_output(self):\n",
    "        assert self.output_names is not None, f\"There are no outputs defined for node: {self.name}\"\n",
    "        if self.partial:\n",
    "            assert len(self.output_names) == 1, \"If the output of node: {self.name} is partial, then there should be one output, {len(self.output_names)} outputs are found\"\n",
    "            self.output = {self.output_names[0]: partial(self.pointer, **self.reference_replaced_arguments)}\n",
    "        else:\n",
    "            output = self.pointer(**self.reference_replaced_arguments)\n",
    "            iterable_output = [output] if len(self.output_names) == 1 else output\n",
    "            self.output = {output_name: output_value for output_name, output_value in zip(self.output_names, iterable_output)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(Path(\"densenet121_two_input_fc_with_tta_template.yml\").open(\"r\"), Loader=PyTorchToolboxLoader)\n",
    "replaced_config = replace_config_variables(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pipeline.create_graph_from_config(replaced_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_X': array([PosixPath('/home/kevin/Documents/Kaggle/human-protein-image-classification/data/test_combined/00008af0-bad0-11e8-b2b8-ac1f6b6435d0.npy'),\n",
       "       PosixPath('/home/kevin/Documents/Kaggle/human-protein-image-classification/data/test_combined/0000a892-bacf-11e8-b2b8-ac1f6b6435d0.npy'),\n",
       "       PosixPath('/home/kevin/Documents/Kaggle/human-protein-image-classification/data/test_combined/0006faa6-bac7-11e8-b2b7-ac1f6b6435d0.npy'),\n",
       "       PosixPath('/home/kevin/Documents/Kaggle/human-protein-image-classification/data/test_combined/0008baca-bad7-11e8-b2b9-ac1f6b6435d0.npy'),\n",
       "       PosixPath('/home/kevin/Documents/Kaggle/human-protein-image-classification/data/test_combined/000cce7e-bad4-11e8-b2b8-ac1f6b6435d0.npy')],\n",
       "      dtype=object)}"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.graph.nodes(data=True)[\"LoadTestingData\"][\"node\"].output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
